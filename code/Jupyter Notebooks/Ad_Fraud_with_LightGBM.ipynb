{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Running on Google Colab might not give Results.***\n",
        "***Needs atleast 30 GB memory. Run this on AWS EMR cluster***"
      ],
      "metadata": {
        "id": "2qwPs5VtKc2L"
      },
      "id": "2qwPs5VtKc2L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   **Installing OpenJDK-8**\n",
        "2.   **Installing Spark-3.0.3 on Hadoop-2.7**\n",
        "3.   **Installing CatBoost Machine Learning Model**\n",
        "4.   **Setting up *JAVA_HOME* and *SPARK_HOME***\n",
        "5.   **Defining Spark Session**\n",
        "6.   **Adding LightGBM Model's Jar Files to Spark Context.**"
      ],
      "metadata": {
        "id": "ePwizdX-bcQj"
      },
      "id": "ePwizdX-bcQj"
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "!pip install -q findspark==1.4.2\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = pyspark.sql.SparkSession.builder.appName(\"FinalProject\") \\\n",
        "            .config(\"spark.jars.packages\", \"com.microsoft.ml.spark:mmlspark_2.12:1.0.0-rc3-49-659b7743-SNAPSHOT\") \\\n",
        "            .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
        "            .getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "xuzmmQrXFuQu",
        "outputId": "c71901b3-08c0-4442-fb7b-ec6ac875316b"
      },
      "id": "xuzmmQrXFuQu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb1d4bbc6a0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://78a047a24f9d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>FinalProject</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries\n",
        "\n",
        "1.   **Installing Visualization Libraries.**\n",
        "2.   **Importing Spark, LightGBM libraries**\n",
        "\n"
      ],
      "metadata": {
        "id": "aaeKvBWN2mJ9"
      },
      "id": "aaeKvBWN2mJ9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91672edd-6d1d-40f1-b38f-6e1da3090600",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91672edd-6d1d-40f1-b38f-6e1da3090600",
        "outputId": "ca103cc0-4fe4-450f-b47c-f030d1f809cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.7.3)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.8/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->seaborn) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.8/dist-packages (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prettytable) (0.2.5)\n"
          ]
        }
      ],
      "source": [
        "#Libraries for Visualization purposesly\n",
        "!pip install seaborn\n",
        "!pip install prettytable\n",
        "\n",
        "#Imports\n",
        "from pyspark.sql.functions import row_number, count, isnan, countDistinct\n",
        "from pyspark.sql.window import *\n",
        "import random\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from pyspark.sql.window import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.types import ArrayType, DoubleType,FloatType\n",
        "from pyspark.sql import Row, functions as F\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler,BucketedRandomProjectionLSH, VectorSlicer, VectorAssembler, StringIndexer, MinMaxScaler\n",
        "from pyspark.sql.functions import col, when, lit, udf, row_number, array, create_map, struct, explode\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#For visualization purposes only\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Used for our main Task\n",
        "# ADDED FOR lightgbm\n",
        "from mmlspark.lightgbm import LightGBMClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Stats Functions\n",
        "\n",
        "1.   **checkNullsInData**: Returns percentage of rows with Null against the Total Rows\n",
        "2.   **checkNullPerTable**: Returns number of Null records full table.\n",
        "3. **getAttributeCount** : Returns the count for each label.\n",
        "4. **getCompleteSummary**: Returns the complete summary of the table."
      ],
      "metadata": {
        "id": "dn-5qZJBhmvW"
      },
      "id": "dn-5qZJBhmvW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "555be072-4526-4d7e-89ab-d0f416caa663",
      "metadata": {
        "id": "555be072-4526-4d7e-89ab-d0f416caa663"
      },
      "outputs": [],
      "source": [
        "# Percentage of rows with Null against the Total Rows\n",
        "def checkNullsInData(data):\n",
        "    # Show how many Null we have in the Dataframe\n",
        "    totalRows = data.count()\n",
        "    drop = data.na.drop().count()\n",
        "    print(\"Total: \", totalRows)\n",
        "    print(\"Left After Dropping:\", drop)\n",
        "    return (totalRows - drop) / totalRows * 100\n",
        "\n",
        "\n",
        "#Number of Nulls full Table\n",
        "def checkNullPerTable(data):\n",
        "    # Show how many Null we have in the Dataframe\n",
        "    print(\"% Of Drops: \",checkNullsInData(data))\n",
        "    # this shows there are a lot of duplicacy in the data.# this shows there are a lot of duplicacy in the data.\n",
        "    # Lets see the percentage: \n",
        "    print(\"% of drop per column\")\n",
        "    return data.select([(count(when(isnan(c) | col(c).isNull(), c))*100/count(lit(1))).alias(c) for c in data.columns])\n",
        "\n",
        "# GET COUNT FOR EACH LABEL\n",
        "def getAttributeCount(data, label=\"is_attributed\"):\n",
        "    print(\"Stats for is attributed:\")\n",
        "    data.groupBy(label).count().show()\n",
        "\n",
        "# GET COMPLETE SUMMARY OF THE DATA\n",
        "def getCompleteSummary(data, label=\"is_attributed\"):\n",
        "    print(\"Summary\")\n",
        "    print(\"_________\")\n",
        "    data.summary().show()\n",
        "    print(\"_________\")\n",
        "    checkNullPerTable(data)\n",
        "    print(\"_________\")\n",
        "    print(\"Unique Values for each column in the table\")\n",
        "    data.agg(*(countDistinct(col(c)).alias(c) for c in data.columns)).show()\n",
        "    print(\"_________\")\n",
        "    print(\"Number of values in is_attributed for each label.\")\n",
        "    print(\"_________\")\n",
        "    getAttributeCount(data, label)\n",
        "    print(\"_________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Utility Functions\n",
        "1. **getFeaturesData**: For returning Vectorized features and label. It also drops exta coulmns if required.\n",
        "2. **findImbalance** : It is used for finding imbalance ratio between the two labels and returns the data Not Fraud, data Fraud, ratio\n",
        "3. **vectorizeData** : Returns Vector Assembled feature by merging all feature columns.\n",
        "4. **stratifiedTrainTestSplit**: It is used for splitting the sampled dataset randomly in 80:20 ratio where 80 is for Training and 20 is for Testing "
      ],
      "metadata": {
        "id": "l9kaZR38cYDo"
      },
      "id": "l9kaZR38cYDo"
    },
    {
      "cell_type": "code",
      "source": [
        "#For returning Vectorized features and label. It also drops exta coulmns if required.\n",
        "def getFeaturesData(\n",
        "    data, inputColumnsList=[\"ip\", \"app\", \"device\", \"os\", \"channel\"], drop=False\n",
        "):\n",
        "    va = VectorAssembler(inputCols=inputColumnsList, outputCol=\"features\")\n",
        "    transformedData = va.transform(data)\n",
        "    if drop:\n",
        "        return (\n",
        "            va.transform(data)\n",
        "            .drop(*inputColumnsList)\n",
        "            .withColumnRenamed(\"is_attributed\", \"label\")\n",
        "        )\n",
        "    return va.transform(data)\n",
        "\n",
        "#It is used for finding imbalance ratio between the two labels and returns the data Not Fraud, data Fraud, ratio\n",
        "def findImbalance(data):\n",
        "    dataNotFraud = data.filter(col(\"is_attributed\") == 0)\n",
        "    dataFraud = data.filter(col(\"is_attributed\") == 1)\n",
        "    countFraud = dataFraud.count()\n",
        "    countNotFraud = dataNotFraud.count()\n",
        "    ratio = int(countNotFraud / countFraud)\n",
        "    print(\n",
        "        \"Count Fraud: {}\\nCount Not Fraud: {}\\nRatio: {}\".format(\n",
        "            countFraud, countNotFraud, ratio\n",
        "        )\n",
        "    )\n",
        "    return dataNotFraud, dataFraud, ratio\n",
        "\n",
        "\n",
        "#Returns Vector Assembled feature by merging all feature columns.\n",
        "def vectorizeData(data, NumericColumns, targetColumn):\n",
        "    if data.select(targetColumn).distinct().count() != 2:\n",
        "        raise ValueError(\"Target col must have exactly 2 classes\")\n",
        "    if targetColumn in NumericColumns:\n",
        "        NumericColumns.remove(targetColumn)\n",
        "    assembler = VectorAssembler(inputCols=NumericColumns, outputCol=\"features\")\n",
        "    vectorizedData = assembler.transform(data)\n",
        "    keepColumns = [a for a in vectorizedData.columns if a not in NumericColumns]\n",
        "    return (\n",
        "        vectorizedData.select(*keepColumns)\n",
        "        .withColumn(\"label\", vectorizedData[targetColumn])\n",
        "        .drop(targetColumn)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "#It is used for splitting the sampled dataset randomly in 80:20 ratio where 80 is for Training and 20 is for Testing\n",
        "def stratifiedTrainTestSplit(data, ifprint=False):\n",
        "    print(\"\\n-----TRAIN TEST SPLIT STARTED----\")\n",
        "    dataNotFraud, dataFraud, ratio= findImbalance(data)\n",
        "    dataNotFraudTrain,dataNotFraudTest=dataNotFraud.randomSplit([0.8, 0.2])\n",
        "    dataFraudTrain,dataFraudTest=dataFraud.randomSplit([0.8, 0.2])\n",
        "    train = dataNotFraudTrain.union(dataFraudTrain)\n",
        "    test = dataFraudTest.union(dataNotFraudTest)\n",
        "    if print:\n",
        "        print(\"\\n----SAMPLES IN TRAIN----\")\n",
        "        dataNotFraud, dataFraud, ratio= findImbalance(train)\n",
        "        print(\"\\n----SAMPLES IN TEST-----\")\n",
        "        dataNotFraud, dataFraud, ratio= findImbalance(test)\n",
        "    return train , test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1O_Ee7w9hwe2"
      },
      "id": "1O_Ee7w9hwe2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Sampling Functions\n",
        "1. **randomOverSample**: It takes the ratio and does random over sampling of the lower count label to match as the higher count label as per the ratio and returns the vectorized data.\n",
        "2. **randomUnderSamplingWithoutTransformation**:  It takes the ratio and does random over sampling of the lower count label to match as the higher count label as per the ratio and returns the data.\n",
        "3. **randomUnderSamplingStratified**: It is used for doing undersampling in a stratified way keeping percentage of labels as per rates r1 and r2, It returns the combined data.\n",
        "4. **randomUnderSampling**:  It takes the ratio and does random under sampling of the decrease the higher count label to match as per the ratio.\n",
        "5. **randomUnderSamplingWithoutTransformation**: It takes the ratio and does random over sampling of the lower count label to match as per the ratio to the higher count label and returns the data.\n",
        "6. **randomUnderSamplingStratified** : It is used for undersampled in a stratified way keeping percentage of labels as per rates r1 and r2, It returns the combined data.\n",
        "7. **randomUnderSampling** : It takes the ratio and does random under sampling of the decrease the higher count label to match as per the ratio.\n",
        "8. **completeOverSampling** : It duplicates the minority class records to match the passed ratio."
      ],
      "metadata": {
        "id": "FQQgTTszfris"
      },
      "id": "FQQgTTszfris"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c7fe2-5038-4a78-8610-607cfbaf274b",
      "metadata": {
        "id": "aa9c7fe2-5038-4a78-8610-607cfbaf274b"
      },
      "outputs": [],
      "source": [
        "# It takes the ratio and does random over sampling of the lower count label to match as the higher count label as per the ratio.\n",
        "def randomOverSample(dataNotFraud, dataFraud, ratio):\n",
        "    dataFraud = dataFraud.sample(True, float(ratio), 24)\n",
        "    totalData = dataFraud.unionAll(dataNotFraud)\n",
        "    return getFeaturesData(totalData, drop=True)\n",
        "\n",
        "#It takes the ratio and does random over sampling of the lower count label to match as per the ratio to the higher count label and returns the data.\n",
        "def randomUnderSamplingWithoutTransformation(dataNotFraud, dataFraud, ratio):\n",
        "    dataNotFraud = dataNotFraud.sample(False, 1 / ratio, 24)\n",
        "    return dataNotFraud.unionAll(dataFraud)\n",
        "\n",
        "# It is used for undersampled in a stratified way keeping percentage of labels as per rates r1 and r2, It returns the combined data.\n",
        "def randomUnderSamplingStratified(data, r1=0.1, r2=0.4):\n",
        "    dataNotFraudSampled = data.filter(col(\"is_attributed\") == 0).sample(False, r1)\n",
        "    dataFraudSampled = data.filter(col(\"is_attributed\") == 1).sample(False, r2)\n",
        "    out = dataNotFraudSampled.union(dataFraudSampled)\n",
        "    return out\n",
        "\n",
        "# It takes the ratio and does random under sampling of the decrease the higher count label to match as per the ratio.\n",
        "def randomUnderSampling(dataNotFraud, dataFraud, ratio):\n",
        "    dataNotFraud = dataNotFraud.sample(False, 1 / ratio, 24)\n",
        "    totalData = dataNotFraud.unionAll(dataFraud)\n",
        "    return getFeaturesData(totalData, drop=True)\n",
        "\n",
        "\n",
        "def completeOverSampling(dataNotFraud, dataFraud, ratio):\n",
        "    a = range(ratio)\n",
        "    # duplicate the minority rows\n",
        "    oversampledData = dataFraud.withColumn(\n",
        "        \"test\", explode(array([lit(x) for x in a]))\n",
        "    ).drop(\"test\")\n",
        "    # combine both oversampled minority rows and previous majority rows combined_df = major_df.unionAll(oversampled_df)\n",
        "    totalData = dataNotFraud.unionAll(oversampledData)\n",
        "    return getFeaturesData(totalData, drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE: Synthetic Minority Over-sampling Technique Implementation\n",
        "\n",
        "1. **checkValidityOfColumnsCheck**: Checking validity of functions, if all columns are correctly type identified.\n",
        "2. **getNumericCategoricalColumns**: Returns the lists of numerical and string columns.\n",
        "3. **smote**: Used above mentioned utlity functions in implementing custom function for SMOTE "
      ],
      "metadata": {
        "id": "rlaSrb1riVhh"
      },
      "id": "rlaSrb1riVhh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Utlity functions of SMOTE\n",
        "\n",
        "\n",
        "#Checking validity of functions, if all columns are correctly type identified.\n",
        "def checkValidityOfColumnsCheck(allColumns, data):\n",
        "    if len(set(allColumns)) == len(data.columns):\n",
        "        print(\"All columns are been covered.\")\n",
        "    elif len(set(allColumns)) < len(data.columns):\n",
        "        not_handle_list = list(set(data.columns) - set(allColumns))\n",
        "        print(\n",
        "            \"Not all columns are covered,The columns missed out: {0}\".format(\n",
        "                not_handle_list\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        mistake_list = list(set(allColumns) - set(data.columns))\n",
        "        print(\"The columns been hardcoded wrongly: {0}\".format(mistake_list))\n",
        "\n",
        "\n",
        "#Returns the lists of numerical and string columns.\n",
        "def getNumericCategoricalColumns(data, excludedList=[]):\n",
        "    timestampColumns = [\n",
        "        item[0] for item in data.dtypes if item[1].lower().startswith((\"time\", \"date\"))\n",
        "    ]\n",
        "    stringColumns = [\n",
        "        item[0]\n",
        "        for item in data.dtypes\n",
        "        if item[1].lower().startswith(\"string\")\n",
        "        and item[0] not in excludedList + timestampColumns\n",
        "    ]\n",
        "    numericColumns = [\n",
        "        item[0]\n",
        "        for item in data.dtypes\n",
        "        if item[1].lower().startswith((\"big\", \"dec\", \"doub\", \"int\", \"float\"))\n",
        "        and item[0] not in excludedList + timestampColumns\n",
        "    ]\n",
        "    allColumns = timestampColumns + stringColumns + numericColumns + excludedList\n",
        "    checkValidityOfColumnsCheck(allColumns, data)\n",
        "    return numericColumns, stringColumns\n",
        "\n",
        "\n",
        "\n",
        "# Synthetic Minority Over-sampling Technique Implementation \n",
        "def smote(dataInit, seed, bucketLength, k, multiplier):\n",
        "    NumericColumns, CatColumns = getNumericCategoricalColumns(dataInit)\n",
        "    data = vectorizeData(dataInit, NumericColumns, targetColumn=\"is_attributed\")\n",
        "    dataInputFraud = data[data[\"label\"] == 1]\n",
        "\n",
        "    # LSH, bucketed random projection\n",
        "    bucketedRandomProjection = BucketedRandomProjectionLSH(\n",
        "        inputCol=\"features\", outputCol=\"hashes\", seed=seed, bucketLength=bucketLength\n",
        "    )\n",
        "    # smote only applies on existing minority instances\n",
        "    model = bucketedRandomProjection.fit(dataInputFraud)\n",
        "    model.transform(dataInputFraud)\n",
        "\n",
        "    # here distance is calculated from bucketedRandomProjection's param inputCol\n",
        "    selfJoinWithDistance = model.approxSimilarityJoin(\n",
        "        dataInputFraud, dataInputFraud, float(\"inf\"), distCol=\"EuclideanDistance\"\n",
        "    )\n",
        "    # remove self-comparison (distance 0)\n",
        "    selfJoinWithDistance = selfJoinWithDistance.filter(\n",
        "        selfJoinWithDistance.EuclideanDistance > 0\n",
        "    )\n",
        "    overOriginalRows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
        "    selfSimilarity = selfJoinWithDistance.withColumn(\n",
        "        \"r_num\", F.row_number().over(overOriginalRows)\n",
        "    )\n",
        "    selfSimilaritySelected = selfSimilarity.filter(selfSimilarity.r_num <= k)\n",
        "    overOriginalRowsNoOrder = Window.partitionBy(\"datasetA\")\n",
        "\n",
        "    # list to store batches of synthetic data\n",
        "    res = []\n",
        "    # two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
        "    subtractVectorUDF = F.udf(\n",
        "        lambda arr: random.uniform(0, 1) * (arr[0] - arr[1]), VectorUDT()\n",
        "    )\n",
        "    addVectorUDF = F.udf(lambda arr: arr[0] + arr[1], VectorUDT())\n",
        "\n",
        "    # retain original columns\n",
        "    originalColumns = dataInputFraud.columns\n",
        "    print(\"Generating New Samples\")\n",
        "    for i in range(multiplier):\n",
        "        # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n",
        "        randomSelectedData = (\n",
        "            selfSimilaritySelected.withColumn(\"rand\", F.rand())\n",
        "            .withColumn(\"max_rand\", F.max(\"rand\").over(overOriginalRowsNoOrder))\n",
        "            .where(F.col(\"rand\") == F.col(\"max_rand\"))\n",
        "            .drop(*[\"max_rand\", \"rand\", \"r_num\"])\n",
        "        )\n",
        "        # create synthetic feature numerical part\n",
        "        vecDiff = randomSelectedData.select(\n",
        "            \"*\",\n",
        "            subtractVectorUDF(F.array(\"datasetA.features\", \"datasetB.features\")).alias(\n",
        "                \"vecdiff\"\n",
        "            ),\n",
        "        )\n",
        "        vecModified = vecDiff.select(\n",
        "            \"*\", addVectorUDF(F.array(\"datasetA.features\", \"vecdiff\")).alias(\"features\")\n",
        "        )\n",
        "        for c in originalColumns:\n",
        "            # randomly select neighbour or original data\n",
        "            colSubsititue = random.choice([\"datasetA\", \"datasetB\"])\n",
        "            val = \"{0}.{1}\".format(colSubsititue, c)\n",
        "            if c != \"features\":\n",
        "                # do not unpack original numerical features\n",
        "                vecModified = vecModified.withColumn(c, F.col(val))\n",
        "        vecModified = vecModified.drop(\n",
        "            *[\"datasetA\", \"datasetB\", \"vecdiff\", \"EuclideanDistance\"]\n",
        "        )\n",
        "        res.append(vecModified)\n",
        "    print(\"Samples Generation Complete.\")\n",
        "\n",
        "    unionedData = reduce(DataFrame.unionAll, res)\n",
        "    # union synthetic instances with original full (both minority and majority) data\n",
        "    return unionedData.union(data.select(unionedData.columns))"
      ],
      "metadata": {
        "id": "H2X4aBKgiUHs"
      },
      "id": "H2X4aBKgiUHs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Model Implementation\n",
        "\n",
        "**LightGBM** : Implementation of LightGBM Classification Model. Returns predictions.\n"
      ],
      "metadata": {
        "id": "WCsqm89EdxGr"
      },
      "id": "WCsqm89EdxGr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a399bff6-c937-4229-9bde-93140b7fdf0d",
      "metadata": {
        "id": "a399bff6-c937-4229-9bde-93140b7fdf0d"
      },
      "outputs": [],
      "source": [
        "def lightGBMC(train,test, isCV=False):\n",
        "    print(\">>> LightGBMClassifier Invoked\")\n",
        "    model = LightGBMClassifier(\n",
        "        objective=\"binary\", featuresCol=\"features\", labelCol=\"label\", isUnbalance=True\n",
        "    )\n",
        "    fitted_model = model.fit(train)\n",
        "    predictions = fitted_model.transform(test)\n",
        "    return {\"predictions\":predictions}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Data Creation\n",
        "\n",
        "**diffSampledData** : Returns the required sampled data upon specification."
      ],
      "metadata": {
        "id": "t-C5WKtMnkNv"
      },
      "id": "t-C5WKtMnkNv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3bca0b-bca2-4922-8db6-4e34b1062c39",
      "metadata": {
        "id": "ac3bca0b-bca2-4922-8db6-4e34b1062c39"
      },
      "outputs": [],
      "source": [
        "def diffSampledData(data, isUnderSample=False,isOverSample=False ,isSMOTE=False, ifprint=False):\n",
        "    sampledData={}\n",
        "    print(\"\\n---Comparing data using various Sampling Techniques---\")\n",
        "\n",
        "    # print(\"\\n--NO Sampling--\")\n",
        "    # sampledData['NO_SAMPLING_APPLIED']=getFeaturesData(data, drop=True)\n",
        "\n",
        "    # Find each class data\n",
        "    dataNotFraud, dataFraud, ratio= findImbalance(data)\n",
        "    if isUnderSample:\n",
        "        print(\"\\n--Undersampling--\")\n",
        "        # Random UnderSample\n",
        "        underSampledData=randomUnderSampling(dataNotFraud,dataFraud,ratio)\n",
        "        # getAttributeCount(underSampledData,\"label\")\n",
        "        sampledData['underSampledData']=underSampledData\n",
        "        \n",
        "    if isOverSample:\n",
        "        print(\"\\n--Random OverSampling--\")\n",
        "        #Random OverSample\n",
        "        randomOverSampleddata=randomOverSample(dataNotFraud,dataFraud,int(ratio*0.75))\n",
        "        # getAttributeCount(randomOverSampleddata,\"label\")\n",
        "        sampledData['randomOverSampleddata']=randomOverSampleddata\n",
        "\n",
        "    # print(\"\\n--Complete OverSampling--\")\n",
        "    # #Complete Oversample\n",
        "    # completeOversampledData=completeOverSampling(dataNotFraud,dataFraud,ratio)\n",
        "    # getAttributeCount(completeOversampledData,\"label\")\n",
        "    # sampledData['completeOversampledData']=completeOversampledData\n",
        "    \n",
        "    if isSMOTE:\n",
        "        print(\"\\n--SMOTE OverSampling--\")\n",
        "        #SMOTE\n",
        "        oversampledDataSMOTE= smote(data, seed=24,bucketLength=200,k=3,multiplier=int(ratio*0.75))\n",
        "        sampledData['oversampledDataSMOTE']=oversampledDataSMOTE\n",
        "    \n",
        "    if ifprint:\n",
        "        if isSMOTE:\n",
        "            print(\"\\n--SMOTE OverSampling--\")\n",
        "            getAttributeCount(oversampledDataSMOTE,\"label\")\n",
        "        if isOverSample:\n",
        "            print(\"\\n--Random OverSampling--\")\n",
        "            getAttributeCount(randomOverSampleddata,\"label\")\n",
        "        # print(\"\\n--Complete OverSampling--\")\n",
        "        # getAttributeCount(completeOversampledData,\"label\")\n",
        "        if isUnderSample:\n",
        "            print(\"\\n--Undersampling--\")\n",
        "            getAttributeCount(underSampledData,\"label\")\n",
        "    return sampledData"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results \n",
        "\n",
        "1. **getResults**: Main method to run the specified Machine Learning models. Return Evaluation metrics. In case of Cross Validation, returns Best Model.\n",
        "\n",
        "2. **filldetails** : Adds all the metrics from different oversampling techniques into table.\n",
        "\n",
        "3. **printConfusionMatrix** ( ***For Visualization purposes only***): Prints confusion Matrix.\n",
        "\n",
        "4. **otherMetrics** : Caluclates Precison, Recall, Accuracy and F1 Score.\n",
        "\n",
        "5. **getEvalutions**: Evaluates predictions with labels and returns the metrics"
      ],
      "metadata": {
        "id": "pQpCcNb4oDQh"
      },
      "id": "pQpCcNb4oDQh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4b8f83-8893-4f06-bd5d-5bce1bd7e4c8",
      "metadata": {
        "id": "fe4b8f83-8893-4f06-bd5d-5bce1bd7e4c8"
      },
      "outputs": [],
      "source": [
        "# Adds all the metrics from different oversampling techniques into table.\n",
        "def filldetails(analysisTable, predictions, sampling, model):\n",
        "    cf_matrix, ROC, accuracy, F1, precision, recall = getEvalutions(predictions)\n",
        "    print(sampling, model, ROC, accuracy, F1, precision, recall, cf_matrix)\n",
        "    analysisTable.add_row([sampling, model, ROC, accuracy, F1, precision, recall, cf_matrix])\n",
        "\n",
        "#Main method to run the specified Machine Learning models. Return Evaluation metrics. In case of Cross Validation, returns Best Model.\n",
        "def getResults(sampledData, test, isLR=False, isRF=False, isLSVC=False, isCatBoost=False, isLightGBM=False,isCV=False):\n",
        "    # Specify the Column Names while initializing the Table\n",
        "    analysisTable = PrettyTable([\"Sampling\", \"Model\", \"ROC\", \"accuracy\", \"F1\", \"precision\", \"recall\", \"Matrix\"])\n",
        "    results = {}\n",
        "    testData = getFeaturesData(test, drop=True)\n",
        "    testData.cache()\n",
        "    for sampling in sampledData:\n",
        "        print(\">>>>>>>>>>>>>>>>Started :\", sampling)\n",
        "        train = sampledData[sampling]\n",
        "        res={}\n",
        "        if isLightGBM:\n",
        "            #LightGBM\n",
        "            modelDataLightGBM = lightGBMC(train, testData, isCV=isCV)\n",
        "            filldetails(analysisTable, modelDataLightGBM[\"predictions\"], sampling, \"LightGBM\")\n",
        "            res[\"LightGBM\"]=modelDataLightGBM\n",
        "        if len(res.keys())>1:\n",
        "            results[sampling] = res\n",
        "        print(\"<<<<<<<<<<<<<<Finished :\", sampling)\n",
        "    return results, analysisTable\n",
        "\n",
        "# print the confusion matrix \n",
        "#USING SEABORN LIBRARY FOR VISUALIZATION PURPOSES\n",
        "def printConfusionMatrix(cf_matrix):\n",
        "    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                    cf_matrix.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "              zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n",
        "\n",
        "\n",
        "#Caluclate F1-score, Recall, Accuaracy, Precision\n",
        "def otherMetrics(cf):\n",
        "    tp = cf[0][0]\n",
        "    fp = cf[1][0]\n",
        "    fn = cf[0][1]\n",
        "    tn = cf[1][1]\n",
        "    precision = np.round((tp)/(tp+fp),3)\n",
        "    recall =  np.round((tp)/(tp+fn),3)\n",
        "    accuracy= np.round((tp+tn)/(tp+fp+fn+tn),3)\n",
        "    F1=np.round((2*precision*recall)/(precision+recall),3)\n",
        "    return accuracy, F1, precision, recall\n",
        "\n",
        "\n",
        "#Evaluate the predictions with actual labels.\n",
        "def getEvalutions(predictions):\n",
        "    evaluator=BinaryClassificationEvaluator(labelCol='label')\n",
        "    ROC = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "    preds_and_labels = predictions.withColumn('label', F.col('label').cast(FloatType())).orderBy('prediction').select(['prediction','label'])\n",
        "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
        "    cf_matrix=metrics.confusionMatrix().toArray()\n",
        "    # printConfusionMatrix(cf_matrix)\n",
        "    accuracy, F1, precision, recall= otherMetrics(cf_matrix)\n",
        "    return cf_matrix, np.round(ROC,3), accuracy, F1, precision, recall"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo \n",
        "**Training and Testing on train_sample.csv data provided along with actual Dataset.**"
      ],
      "metadata": {
        "id": "zhzUNW4ioIt7"
      },
      "id": "zhzUNW4ioIt7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03625b8a-b5fc-47f0-baf1-950a9d883acd",
      "metadata": {
        "id": "03625b8a-b5fc-47f0-baf1-950a9d883acd"
      },
      "outputs": [],
      "source": [
        "def demoData(path=\"/content/drive/MyDrive/talkingdata-adtracking-fraud-detection/train_sample.csv\"):\n",
        "    dataDownload = spark.read\\\n",
        "      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
        "      .option(\"inferSchema\",True)\\\n",
        "      .option('header', 'true')\\\n",
        "      .load(path).drop(\"attributed_time\",\"click_time\").distinct().na.drop()\n",
        "    dataNotFraud, dataFraud, ratio= findImbalance(dataDownload)\n",
        "    print(\"\\n--Undersampling to create demo set--\")\n",
        "    # Random UnderSample the big data to form processable ratio for demo.\n",
        "    underSampledData=randomUnderSamplingWithoutTransformation(dataNotFraud,dataFraud,int(ratio/8))\n",
        "    getCompleteSummary(underSampledData)\n",
        "    trainSample,testSample=stratifiedTrainTestSplit(underSampledData, ifprint=False)\n",
        "    sampledData=diffSampledData(trainSample,isUnderSample=True,isOverSample=False ,isSMOTE=False, ifprint=False)\n",
        "    results, analysisTable= getResults(sampledData,testSample,isLightGBM=True)\n",
        "    print(\"\\n________________RESULTS______________\\n\",analysisTable)\n",
        "    return results, analysisTable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultsDemo, analysisTableDemo= demoData(path='/content/drive/MyDrive/Final Project CS 657/talkingdata-adtracking-fraud-detection/train_sample.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OE_L0gdGGuPC",
        "outputId": "4bd2732c-d69c-4f8a-c537-29d9d85e524f"
      },
      "id": "OE_L0gdGGuPC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Fraud: 227\n",
            "Count Not Fraud: 97693\n",
            "Ratio: 430\n",
            "\n",
            "--Undersampling to create demo set--\n",
            "Summary\n",
            "_________\n",
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+\n",
            "|summary|                ip|               app|            device|               os|           channel|      is_attributed|\n",
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+\n",
            "|  count|              2087|              2087|              2087|             2087|              2087|               2087|\n",
            "|   mean|100230.09439386679|13.959271681839962| 23.43938667944418|24.22376617153809|261.14278869190224|0.10876856732151413|\n",
            "| stddev| 77739.94495546671|17.928155573536145|260.76996773054475| 63.0199579597241|129.28617993665284|0.31142324387481285|\n",
            "|    min|                31|                 1|                 0|                0|                 3|                  0|\n",
            "|    25%|             42289|                 3|                 1|               13|               137|                  0|\n",
            "|    50%|             86812|                12|                 1|               18|               245|                  0|\n",
            "|    75%|            125175|                18|                 1|               20|               377|                  0|\n",
            "|    max|            363169|               261|              3866|              866|               497|                  1|\n",
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+\n",
            "\n",
            "_________\n",
            "Total:  2087\n",
            "Left After Dropping: 2087\n",
            "% Of Drops:  0.0\n",
            "% of drop per column\n",
            "_________\n",
            "Unique Values for each column in the table\n",
            "+----+---+------+---+-------+-------------+\n",
            "|  ip|app|device| os|channel|is_attributed|\n",
            "+----+---+------+---+-------+-------------+\n",
            "|1913| 65|    30| 64|    126|            2|\n",
            "+----+---+------+---+-------+-------------+\n",
            "\n",
            "_________\n",
            "Number of values in is_attributed for each label.\n",
            "_________\n",
            "Stats for is attributed:\n",
            "+-------------+-----+\n",
            "|is_attributed|count|\n",
            "+-------------+-----+\n",
            "|            1|  227|\n",
            "|            0| 1860|\n",
            "+-------------+-----+\n",
            "\n",
            "_________\n",
            "\n",
            "-----TRAIN TEST SPLIT STARTED----\n",
            "Count Fraud: 227\n",
            "Count Not Fraud: 1860\n",
            "Ratio: 8\n",
            "\n",
            "----SAMPLES IN TRAIN----\n",
            "Count Fraud: 192\n",
            "Count Not Fraud: 1497\n",
            "Ratio: 7\n",
            "\n",
            "----SAMPLES IN TEST-----\n",
            "Count Fraud: 35\n",
            "Count Not Fraud: 363\n",
            "Ratio: 10\n",
            "\n",
            "---Comparing data using various Sampling Techniques---\n",
            "Count Fraud: 192\n",
            "Count Not Fraud: 1497\n",
            "Ratio: 7\n",
            "\n",
            "--Undersampling--\n",
            ">>>>>>>>>>>>>>>>Started : underSampledData\n",
            ">>> LightGBMClassifier Invoked\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1211, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while receiving\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-42289262306b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresultsDemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysisTableDemo\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdemoData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Final Project CS 657/talkingdata-adtracking-fraud-detection/train_sample.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-3dd03c57ff22>\u001b[0m in \u001b[0;36mdemoData\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrainSample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestSample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratifiedTrainTestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munderSampledData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msampledData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiffSampledData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misUnderSample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misOverSample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0misSMOTE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysisTable\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgetResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampledData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestSample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misLightGBM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n________________RESULTS______________\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalysisTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysisTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-778152a70af9>\u001b[0m in \u001b[0;36mgetResults\u001b[0;34m(sampledData, test, isLR, isRF, isLSVC, isCatBoost, isLightGBM, isCV)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misLightGBM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#LightGBM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmodelDataLightGBM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlightGBMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misCV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0misCV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mfilldetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysisTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelDataLightGBM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LightGBM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LightGBM\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelDataLightGBM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-48220fc88679>\u001b[0m in \u001b[0;36mlightGBMC\u001b[0;34m(train, test, isCV)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misUnbalance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfitted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitted_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o317.fit"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Million Records\n",
        "**Training and Testing on sampled 6 Million records from train.csv**"
      ],
      "metadata": {
        "id": "QAyfEaTRoJ3B"
      },
      "id": "QAyfEaTRoJ3B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987c915d-b952-49c5-9fe6-3ff025290568",
      "metadata": {
        "id": "987c915d-b952-49c5-9fe6-3ff025290568"
      },
      "outputs": [],
      "source": [
        "def RUN6MTEST(path=\"../Data/Sampled_data.parquet\"):\n",
        "    dataDownload=spark.read.parquet(path)\n",
        "    getCompleteSummary(dataDownload)\n",
        "    trainSample,testSample=stratifiedTrainTestSplit(dataDownload, ifprint=False)\n",
        "    sampledData=diffSampledData(trainSample, isUnderSample=False,isOverSample=True, isSMOTE=False, ifprint=False)\n",
        "    results, analysisTable= getResults(sampledData,testSample,isLightGBM=True)\n",
        "    print(\"\\n________________RESULTS______________\\n\",analysisTable)\n",
        "    return results, analysisTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744cb95e-306a-4762-b1e4-9af5d7b7405c",
      "metadata": {
        "id": "744cb95e-306a-4762-b1e4-9af5d7b7405c"
      },
      "outputs": [],
      "source": [
        "results6M, analysisTable6M= RUN6MTEST()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 26 Million Records\n",
        "**Training and Testing on sampled 26 Million records from train.csv**\n"
      ],
      "metadata": {
        "id": "eV3wF56ZoKi4"
      },
      "id": "eV3wF56ZoKi4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4b3a46-b9f6-4241-a79d-fc99bf60ef80",
      "metadata": {
        "id": "4c4b3a46-b9f6-4241-a79d-fc99bf60ef80"
      },
      "outputs": [],
      "source": [
        "def RUN26MTEST(path=\"../Data/Sampled25M.parquet\"):\n",
        "    dataDownload=spark.read.parquet(path)\n",
        "    getCompleteSummary(dataDownload)\n",
        "    trainSample,testSample=stratifiedTrainTestSplit(dataDownload, ifprint=False)\n",
        "    sampledData=diffSampledData(trainSample, isUnderSample=False,isOverSample=True, ifprint=False)\n",
        "    results, analysisTable= getResults(sampledData,testSample,isLightGBM=True)\n",
        "    print(\"\\n________________RESULTS______________\\n\",analysisTable)\n",
        "    return results, analysisTable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results26M, analysisTable26M= RUN26MTEST()"
      ],
      "metadata": {
        "id": "l6_nARQiaZwu"
      },
      "id": "l6_nARQiaZwu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}