{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Connect with Google Drive for loading Data.**"
      ],
      "metadata": {
        "id": "iDyOIjghbRhv"
      },
      "id": "iDyOIjghbRhv"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OP3DKRjFr7h",
        "outputId": "292b7aee-5b58-4ca2-99d9-ca2d5f5b1eeb"
      },
      "id": "8OP3DKRjFr7h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   **Installing OpenJDK-8**\n",
        "2.   **Installing Spark-3.0.3 on Hadoop-2.7**\n",
        "3.   **Installing CatBoost Machine Learning Model**\n",
        "4.   **Setting up *JAVA_HOME* and *SPARK_HOME***\n",
        "5.   **Defining Spark Session**\n",
        "6.   **Adding CatBoost Model's Jar Files to Spark Context.**"
      ],
      "metadata": {
        "id": "ePwizdX-bcQj"
      },
      "id": "ePwizdX-bcQj"
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark==1.4.2 catboost==1.0.3\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "    .master('local[*]')\\\n",
        "    .appName('CatBoostWithPySparkFinalProject')\\\n",
        "    .config(\"spark.jars.packages\", \"ai.catboost:catboost-spark_3.0_2.12:1.0.3\")\\\n",
        "    .config(\"spark.executor.cores\", \"2\")\\\n",
        "    .config(\"spark.task.cpus\", \"2\")\\\n",
        "    .config(\"spark.driver.memory\", \"2g\")\\\n",
        "    .config(\"spark.driver.memoryOverhead\", \"2g\")\\\n",
        "    .config(\"spark.executor.memory\", \"2g\")\\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"2g\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n",
        "import catboost_spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "xuzmmQrXFuQu",
        "outputId": "ba47d82b-b6ed-4934-9376-5ffb187ecb41"
      },
      "id": "xuzmmQrXFuQu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 76.3 MB 1.2 MB/s \n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f35795dfbe0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://62c75294e8bc:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>CatBoostWithPySparkFinalProject</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries\n",
        "\n",
        "1.   **Installing Visualization Libraries.**\n",
        "2.   **Importing Spark, Catboost libraries**\n",
        "\n"
      ],
      "metadata": {
        "id": "aaeKvBWN2mJ9"
      },
      "id": "aaeKvBWN2mJ9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91672edd-6d1d-40f1-b38f-6e1da3090600",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91672edd-6d1d-40f1-b38f-6e1da3090600",
        "outputId": "f5bdc0ac-a6b2-4e41-9de2-eddd79bb8e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (0.11.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.8/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->seaborn) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.8/dist-packages (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prettytable) (0.2.5)\n"
          ]
        }
      ],
      "source": [
        "#Installing Visualization Libraries Seaborn and PrettyTable\n",
        "!pip install seaborn\n",
        "!pip install prettytable\n",
        "\n",
        "#IMPORTS\n",
        "from pyspark.sql.functions import row_number, count, isnan, countDistinct\n",
        "from pyspark.sql.window import *\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from functools import reduce\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.window import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.types import ArrayType, DoubleType,FloatType\n",
        "from pyspark.sql import Row, functions as F\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler,BucketedRandomProjectionLSH, VectorSlicer, VectorAssembler, StringIndexer, MinMaxScaler\n",
        "from pyspark.sql.functions import col, when, lit, udf, row_number, array, create_map, struct, explode\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from prettytable import PrettyTable\n",
        "import catboost_spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Stats Functions\n",
        "\n",
        "\n",
        "1.   **checkNullsInData**: Returns percentage of rows with Null against the Total Rows\n",
        "2.   **checkNullPerTable**: Returns number of Null records full table.\n",
        "3. **getAttributeCount** : Returns the count for each label.\n",
        "4. **getCompleteSummary**: Returns the complete summary of the table.\n",
        "\n"
      ],
      "metadata": {
        "id": "CZq4DPiygaKq"
      },
      "id": "CZq4DPiygaKq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "555be072-4526-4d7e-89ab-d0f416caa663",
      "metadata": {
        "id": "555be072-4526-4d7e-89ab-d0f416caa663"
      },
      "outputs": [],
      "source": [
        "# Percentage of rows with Null against the Total Rows\n",
        "def checkNullsInData(data):\n",
        "    # Show how many Null we have in the Dataframe\n",
        "    totalRows = data.count()\n",
        "    drop = data.na.drop().count()\n",
        "    print(\"Total: \", totalRows)\n",
        "    print(\"Left After Dropping:\", drop)\n",
        "    return (totalRows - drop) / totalRows * 100\n",
        "\n",
        "\n",
        "#Number of Nulls full Table\n",
        "def checkNullPerTable(data):\n",
        "    # Show how many Null we have in the Dataframe\n",
        "    print(\"% Of Drops: \",checkNullsInData(data))\n",
        "    # this shows there are a lot of duplicacy in the data.# this shows there are a lot of duplicacy in the data.\n",
        "    # Lets see the percentage: \n",
        "    print(\"% of drop per column\")\n",
        "    return data.select([(count(when(isnan(c) | col(c).isNull(), c))*100/count(lit(1))).alias(c) for c in data.columns])\n",
        "\n",
        "# GET COUNT FOR EACH LABEL\n",
        "def getAttributeCount(data, label=\"is_attributed\"):\n",
        "    print(\"Stats for is attributed:\")\n",
        "    data.groupBy(label).count().show()\n",
        "\n",
        "# GET COMPLETE SUMMARY OF THE DATA\n",
        "def getCompleteSummary(data, label=\"is_attributed\"):\n",
        "    print(\"Summary\")\n",
        "    print(\"_________\")\n",
        "    data.summary().show()\n",
        "    print(\"_________\")\n",
        "    checkNullPerTable(data)\n",
        "    print(\"_________\")\n",
        "    print(\"Unique Values for each column in the table\")\n",
        "    data.agg(*(countDistinct(col(c)).alias(c) for c in data.columns)).show()\n",
        "    print(\"_________\")\n",
        "    print(\"Number of values in is_attributed for each label.\")\n",
        "    print(\"_________\")\n",
        "    getAttributeCount(data, label)\n",
        "    print(\"_________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Utility Functions\n",
        "1. **getFeaturesData**: For returning Vectorized features and label. It also drops exta coulmns if required.\n",
        "2. **findImbalance** : It is used for finding imbalance ratio between the two labels and returns the data Not Fraud, data Fraud, ratio\n",
        "3. **vectorizeData** : Returns Vector Assembled feature by merging all feature columns.\n",
        "4. **stratifiedTrainTestSplit**: It is used for splitting the sampled dataset randomly in 80:20 ratio where 80 is for Training and 20 is for Testing "
      ],
      "metadata": {
        "id": "l9kaZR38cYDo"
      },
      "id": "l9kaZR38cYDo"
    },
    {
      "cell_type": "code",
      "source": [
        "#For returning Vectorized features and label. It also drops exta coulmns if required.\n",
        "def getFeaturesData(\n",
        "    data, inputColumnsList=[\"ip\", \"app\", \"device\", \"os\", \"channel\"], drop=False\n",
        "):\n",
        "    va = VectorAssembler(inputCols=inputColumnsList, outputCol=\"features\")\n",
        "    transformedData = va.transform(data)\n",
        "    if drop:\n",
        "        return (\n",
        "            va.transform(data)\n",
        "            .drop(*inputColumnsList)\n",
        "            .withColumnRenamed(\"is_attributed\", \"label\")\n",
        "        )\n",
        "    return va.transform(data)\n",
        "\n",
        "#It is used for finding imbalance ratio between the two labels and returns the data Not Fraud, data Fraud, ratio\n",
        "def findImbalance(data):\n",
        "    dataNotFraud = data.filter(col(\"is_attributed\") == 0)\n",
        "    dataFraud = data.filter(col(\"is_attributed\") == 1)\n",
        "    countFraud = dataFraud.count()\n",
        "    countNotFraud = dataNotFraud.count()\n",
        "    ratio = int(countNotFraud / countFraud)\n",
        "    print(\n",
        "        \"Count Fraud: {}\\nCount Not Fraud: {}\\nRatio: {}\".format(\n",
        "            countFraud, countNotFraud, ratio\n",
        "        )\n",
        "    )\n",
        "    return dataNotFraud, dataFraud, ratio\n",
        "\n",
        "\n",
        "#Returns Vector Assembled feature by merging all feature columns.\n",
        "def vectorizeData(data, NumericColumns, targetColumn):\n",
        "    if data.select(targetColumn).distinct().count() != 2:\n",
        "        raise ValueError(\"Target col must have exactly 2 classes\")\n",
        "    if targetColumn in NumericColumns:\n",
        "        NumericColumns.remove(targetColumn)\n",
        "    assembler = VectorAssembler(inputCols=NumericColumns, outputCol=\"features\")\n",
        "    vectorizedData = assembler.transform(data)\n",
        "    keepColumns = [a for a in vectorizedData.columns if a not in NumericColumns]\n",
        "    return (\n",
        "        vectorizedData.select(*keepColumns)\n",
        "        .withColumn(\"label\", vectorizedData[targetColumn])\n",
        "        .drop(targetColumn)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "#It is used for splitting the sampled dataset randomly in 80:20 ratio where 80 is for Training and 20 is for Testing\n",
        "def stratifiedTrainTestSplit(data, ifprint=False):\n",
        "    print(\"\\n-----TRAIN TEST SPLIT STARTED----\")\n",
        "    dataNotFraud, dataFraud, ratio= findImbalance(data)\n",
        "    dataNotFraudTrain,dataNotFraudTest=dataNotFraud.randomSplit([0.8, 0.2])\n",
        "    dataFraudTrain,dataFraudTest=dataFraud.randomSplit([0.8, 0.2])\n",
        "    train = dataNotFraudTrain.union(dataFraudTrain)\n",
        "    test = dataFraudTest.union(dataNotFraudTest)\n",
        "    if print:\n",
        "        print(\"\\n----SAMPLES IN TRAIN----\")\n",
        "        dataNotFraud, dataFraud, ratio= findImbalance(train)\n",
        "        print(\"\\n----SAMPLES IN TEST-----\")\n",
        "        dataNotFraud, dataFraud, ratio= findImbalance(test)\n",
        "    return train , test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1O_Ee7w9hwe2"
      },
      "id": "1O_Ee7w9hwe2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Sampling Functions\n",
        "1. **randomOverSample**: It takes the ratio and does random over sampling of the lower count label to match as the higher count label as per the ratio and returns the vectorized data.\n",
        "2. **randomUnderSamplingWithoutTransformation**:  It takes the ratio and does random over sampling of the lower count label to match as the higher count label as per the ratio and returns the data.\n",
        "3. **randomUnderSamplingStratified**: It is used for doing undersampling in a stratified way keeping percentage of labels as per rates r1 and r2, It returns the combined data.\n",
        "4. **randomUnderSampling**:  It takes the ratio and does random under sampling of the decrease the higher count label to match as per the ratio.\n",
        "5. **randomUnderSamplingWithoutTransformation**: It takes the ratio and does random over sampling of the lower count label to match as per the ratio to the higher count label and returns the data.\n",
        "6. **randomUnderSamplingStratified** : It is used for undersampled in a stratified way keeping percentage of labels as per rates r1 and r2, It returns the combined data.\n",
        "7. **randomUnderSampling** : It takes the ratio and does random under sampling of the decrease the higher count label to match as per the ratio.\n",
        "8. **completeOverSampling** : It duplicates the minority class records to match the passed ratio."
      ],
      "metadata": {
        "id": "FQQgTTszfris"
      },
      "id": "FQQgTTszfris"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c7fe2-5038-4a78-8610-607cfbaf274b",
      "metadata": {
        "id": "aa9c7fe2-5038-4a78-8610-607cfbaf274b"
      },
      "outputs": [],
      "source": [
        "# It takes the ratio and does random over sampling of the lower count label to match as the higher count label as per the ratio.\n",
        "def randomOverSample(dataNotFraud, dataFraud, ratio):\n",
        "    dataFraud = dataFraud.sample(True, float(ratio), 24)\n",
        "    totalData = dataFraud.unionAll(dataNotFraud)\n",
        "    return getFeaturesData(totalData, drop=True)\n",
        "\n",
        "#It takes the ratio and does random over sampling of the lower count label to match as per the ratio to the higher count label and returns the data.\n",
        "def randomUnderSamplingWithoutTransformation(dataNotFraud, dataFraud, ratio):\n",
        "    dataNotFraud = dataNotFraud.sample(False, 1 / ratio, 24)\n",
        "    return dataNotFraud.unionAll(dataFraud)\n",
        "\n",
        "# It is used for undersampled in a stratified way keeping percentage of labels as per rates r1 and r2, It returns the combined data.\n",
        "def randomUnderSamplingStratified(data, r1=0.1, r2=0.4):\n",
        "    dataNotFraudSampled = data.filter(col(\"is_attributed\") == 0).sample(False, r1)\n",
        "    dataFraudSampled = data.filter(col(\"is_attributed\") == 1).sample(False, r2)\n",
        "    out = dataNotFraudSampled.union(dataFraudSampled)\n",
        "    return out\n",
        "\n",
        "# It takes the ratio and does random under sampling of the decrease the higher count label to match as per the ratio.\n",
        "def randomUnderSampling(dataNotFraud, dataFraud, ratio):\n",
        "    dataNotFraud = dataNotFraud.sample(False, 1 / ratio, 24)\n",
        "    totalData = dataNotFraud.unionAll(dataFraud)\n",
        "    return getFeaturesData(totalData, drop=True)\n",
        "\n",
        "\n",
        "def completeOverSampling(dataNotFraud, dataFraud, ratio):\n",
        "    a = range(ratio)\n",
        "    # duplicate the minority rows\n",
        "    oversampledData = dataFraud.withColumn(\n",
        "        \"test\", explode(array([lit(x) for x in a]))\n",
        "    ).drop(\"test\")\n",
        "    # combine both oversampled minority rows and previous majority rows combined_df = major_df.unionAll(oversampled_df)\n",
        "    totalData = dataNotFraud.unionAll(oversampledData)\n",
        "    return getFeaturesData(totalData, drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE: Synthetic Minority Over-sampling Technique Implementation\n",
        "\n",
        "1. **checkValidityOfColumnsCheck**: Checking validity of functions, if all columns are correctly type identified.\n",
        "2. **getNumericCategoricalColumns**: Returns the lists of numerical and string columns.\n",
        "3. **smote**: Used above mentioned utlity functions in implementing custom function for SMOTE "
      ],
      "metadata": {
        "id": "rlaSrb1riVhh"
      },
      "id": "rlaSrb1riVhh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Utlity functions of SMOTE\n",
        "\n",
        "\n",
        "#Checking validity of functions, if all columns are correctly type identified.\n",
        "def checkValidityOfColumnsCheck(allColumns, data):\n",
        "    if len(set(allColumns)) == len(data.columns):\n",
        "        print(\"All columns are been covered.\")\n",
        "    elif len(set(allColumns)) < len(data.columns):\n",
        "        not_handle_list = list(set(data.columns) - set(allColumns))\n",
        "        print(\n",
        "            \"Not all columns are covered,The columns missed out: {0}\".format(\n",
        "                not_handle_list\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        mistake_list = list(set(allColumns) - set(data.columns))\n",
        "        print(\"The columns been hardcoded wrongly: {0}\".format(mistake_list))\n",
        "\n",
        "\n",
        "#Returns the lists of numerical and string columns.\n",
        "def getNumericCategoricalColumns(data, excludedList=[]):\n",
        "    timestampColumns = [\n",
        "        item[0] for item in data.dtypes if item[1].lower().startswith((\"time\", \"date\"))\n",
        "    ]\n",
        "    stringColumns = [\n",
        "        item[0]\n",
        "        for item in data.dtypes\n",
        "        if item[1].lower().startswith(\"string\")\n",
        "        and item[0] not in excludedList + timestampColumns\n",
        "    ]\n",
        "    numericColumns = [\n",
        "        item[0]\n",
        "        for item in data.dtypes\n",
        "        if item[1].lower().startswith((\"big\", \"dec\", \"doub\", \"int\", \"float\"))\n",
        "        and item[0] not in excludedList + timestampColumns\n",
        "    ]\n",
        "    allColumns = timestampColumns + stringColumns + numericColumns + excludedList\n",
        "    checkValidityOfColumnsCheck(allColumns, data)\n",
        "    return numericColumns, stringColumns\n",
        "\n",
        "\n",
        "\n",
        "# Synthetic Minority Over-sampling Technique Implementation \n",
        "def smote(dataInit, seed, bucketLength, k, multiplier):\n",
        "    NumericColumns, CatColumns = getNumericCategoricalColumns(dataInit)\n",
        "    data = vectorizeData(dataInit, NumericColumns, targetColumn=\"is_attributed\")\n",
        "    dataInputFraud = data[data[\"label\"] == 1]\n",
        "\n",
        "    # LSH, bucketed random projection\n",
        "    bucketedRandomProjection = BucketedRandomProjectionLSH(\n",
        "        inputCol=\"features\", outputCol=\"hashes\", seed=seed, bucketLength=bucketLength\n",
        "    )\n",
        "    # smote only applies on existing minority instances\n",
        "    model = bucketedRandomProjection.fit(dataInputFraud)\n",
        "    model.transform(dataInputFraud)\n",
        "\n",
        "    # here distance is calculated from bucketedRandomProjection's param inputCol\n",
        "    selfJoinWithDistance = model.approxSimilarityJoin(\n",
        "        dataInputFraud, dataInputFraud, float(\"inf\"), distCol=\"EuclideanDistance\"\n",
        "    )\n",
        "    # remove self-comparison (distance 0)\n",
        "    selfJoinWithDistance = selfJoinWithDistance.filter(\n",
        "        selfJoinWithDistance.EuclideanDistance > 0\n",
        "    )\n",
        "    overOriginalRows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
        "    selfSimilarity = selfJoinWithDistance.withColumn(\n",
        "        \"r_num\", F.row_number().over(overOriginalRows)\n",
        "    )\n",
        "    selfSimilaritySelected = selfSimilarity.filter(selfSimilarity.r_num <= k)\n",
        "    overOriginalRowsNoOrder = Window.partitionBy(\"datasetA\")\n",
        "\n",
        "    # list to store batches of synthetic data\n",
        "    res = []\n",
        "    # two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
        "    subtractVectorUDF = F.udf(\n",
        "        lambda arr: random.uniform(0, 1) * (arr[0] - arr[1]), VectorUDT()\n",
        "    )\n",
        "    addVectorUDF = F.udf(lambda arr: arr[0] + arr[1], VectorUDT())\n",
        "\n",
        "    # retain original columns\n",
        "    originalColumns = dataInputFraud.columns\n",
        "    print(\"Generating New Samples\")\n",
        "    for i in range(multiplier):\n",
        "        # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n",
        "        randomSelectedData = (\n",
        "            selfSimilaritySelected.withColumn(\"rand\", F.rand())\n",
        "            .withColumn(\"max_rand\", F.max(\"rand\").over(overOriginalRowsNoOrder))\n",
        "            .where(F.col(\"rand\") == F.col(\"max_rand\"))\n",
        "            .drop(*[\"max_rand\", \"rand\", \"r_num\"])\n",
        "        )\n",
        "        # create synthetic feature numerical part\n",
        "        vecDiff = randomSelectedData.select(\n",
        "            \"*\",\n",
        "            subtractVectorUDF(F.array(\"datasetA.features\", \"datasetB.features\")).alias(\n",
        "                \"vecdiff\"\n",
        "            ),\n",
        "        )\n",
        "        vecModified = vecDiff.select(\n",
        "            \"*\", addVectorUDF(F.array(\"datasetA.features\", \"vecdiff\")).alias(\"features\")\n",
        "        )\n",
        "        for c in originalColumns:\n",
        "            # randomly select neighbour or original data\n",
        "            colSubsititue = random.choice([\"datasetA\", \"datasetB\"])\n",
        "            val = \"{0}.{1}\".format(colSubsititue, c)\n",
        "            if c != \"features\":\n",
        "                # do not unpack original numerical features\n",
        "                vecModified = vecModified.withColumn(c, F.col(val))\n",
        "        vecModified = vecModified.drop(\n",
        "            *[\"datasetA\", \"datasetB\", \"vecdiff\", \"EuclideanDistance\"]\n",
        "        )\n",
        "        res.append(vecModified)\n",
        "    print(\"Samples Generation Complete.\")\n",
        "\n",
        "    unionedData = reduce(DataFrame.unionAll, res)\n",
        "    # union synthetic instances with original full (both minority and majority) data\n",
        "    return unionedData.union(data.select(unionedData.columns))"
      ],
      "metadata": {
        "id": "H2X4aBKgiUHs"
      },
      "id": "H2X4aBKgiUHs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Models Implementation\n",
        "\n",
        "1. **CatBoost**: Implements Categotical Boosting Machine Learning by using the python wrapper code and jar files which we added to spark context in the **Setup** section. This implements Cross Calidation."
      ],
      "metadata": {
        "id": "WCsqm89EdxGr"
      },
      "id": "WCsqm89EdxGr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a399bff6-c937-4229-9bde-93140b7fdf0d",
      "metadata": {
        "id": "a399bff6-c937-4229-9bde-93140b7fdf0d"
      },
      "outputs": [],
      "source": [
        "#implementing CatBoost Machine Learning Algorithm.\n",
        "def CatBoost(train,test,isCV=False):\n",
        "    print(\">>> CatBoost Invoked\")\n",
        "    train_pool = catboost_spark.Pool(train.select(['features', 'label']))\n",
        "    train_pool.setLabelCol('label')\n",
        "    train_pool.setFeaturesCol('features')\n",
        "    evaluator=BinaryClassificationEvaluator(labelCol='label')\n",
        "    catboost= catboost_spark.CatBoostClassifier(featuresCol='features', labelCol='label', iterations=50,depth=5)\n",
        "    if isCV:\n",
        "        fold=10\n",
        "        paramGrid = (ParamGridBuilder().addGrid(catboost.depth, [5, 15, 30]) \\\n",
        "                                    .addGrid(catboost.iterations, [10,50,100])\n",
        "                                    .build())\n",
        "        cv = CrossValidator(estimator=catboost,\n",
        "                                    estimatorParamMaps=paramGrid,\n",
        "                                    evaluator=evaluator,\n",
        "                                    numFolds=fold) \n",
        "        cv_model = cv.fit(train_pool)\n",
        "        bestModel = cv_model.bestModel\n",
        "        cv_predictions = cv_model.transform(test)\n",
        "        return {\"predictions\":cv_predictions, 'bestModel':bestModel}\n",
        "    else:\n",
        "      model = catboost.fit(train_pool)\n",
        "      predictions = model.transform(test)\n",
        "      return {\"predictions\":predictions}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Data Creation\n",
        "\n",
        "**diffSampledData** : Returns the required sampled data upon specification."
      ],
      "metadata": {
        "id": "t-C5WKtMnkNv"
      },
      "id": "t-C5WKtMnkNv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3bca0b-bca2-4922-8db6-4e34b1062c39",
      "metadata": {
        "id": "ac3bca0b-bca2-4922-8db6-4e34b1062c39"
      },
      "outputs": [],
      "source": [
        "def diffSampledData(data, isUnderSample=False,isOverSample=False ,isSMOTE=False, ifprint=False):\n",
        "    sampledData={}\n",
        "    print(\"\\n---Comparing data using various Sampling Techniques---\")\n",
        "\n",
        "    # print(\"\\n--NO Sampling--\")\n",
        "    # sampledData['NO_SAMPLING_APPLIED']=getFeaturesData(data, drop=True)\n",
        "\n",
        "    # Find each class data\n",
        "    dataNotFraud, dataFraud, ratio= findImbalance(data)\n",
        "    if isUnderSample:\n",
        "        print(\"\\n--Undersampling--\")\n",
        "        # Random UnderSample\n",
        "        underSampledData=randomUnderSampling(dataNotFraud,dataFraud,ratio)\n",
        "        # getAttributeCount(underSampledData,\"label\")\n",
        "        sampledData['underSampledData']=underSampledData\n",
        "        \n",
        "    if isOverSample:\n",
        "        print(\"\\n--Random OverSampling--\")\n",
        "        #Random OverSample\n",
        "        randomOverSampleddata=randomOverSample(dataNotFraud,dataFraud,int(ratio*0.75))\n",
        "        # getAttributeCount(randomOverSampleddata,\"label\")\n",
        "        sampledData['randomOverSampleddata']=randomOverSampleddata\n",
        "\n",
        "    # print(\"\\n--Complete OverSampling--\")\n",
        "    # #Complete Oversample\n",
        "    # completeOversampledData=completeOverSampling(dataNotFraud,dataFraud,ratio)\n",
        "    # getAttributeCount(completeOversampledData,\"label\")\n",
        "    # sampledData['completeOversampledData']=completeOversampledData\n",
        "    \n",
        "    if isSMOTE:\n",
        "        print(\"\\n--SMOTE OverSampling--\")\n",
        "        #SMOTE\n",
        "        oversampledDataSMOTE= smote(data, seed=24,bucketLength=200,k=3,multiplier=int(ratio*0.75))\n",
        "        sampledData['oversampledDataSMOTE']=oversampledDataSMOTE\n",
        "    \n",
        "    if ifprint:\n",
        "        if isSMOTE:\n",
        "            print(\"\\n--SMOTE OverSampling--\")\n",
        "            getAttributeCount(oversampledDataSMOTE,\"label\")\n",
        "        if isOverSample:\n",
        "            print(\"\\n--Random OverSampling--\")\n",
        "            getAttributeCount(randomOverSampleddata,\"label\")\n",
        "        # print(\"\\n--Complete OverSampling--\")\n",
        "        # getAttributeCount(completeOversampledData,\"label\")\n",
        "        if isUnderSample:\n",
        "            print(\"\\n--Undersampling--\")\n",
        "            getAttributeCount(underSampledData,\"label\")\n",
        "    return sampledData"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results \n",
        "\n",
        "1. **getResults**: Main method to run the specified Machine Learning models. Return Evaluation metrics. In case of Cross Validation, returns Best Model.\n",
        "\n",
        "2. **filldetails** : Adds all the metrics from different oversampling techniques into table.\n",
        "\n",
        "3. **printConfusionMatrix** ( ***For Visualization purposes only***): Prints confusion Matrix.\n",
        "\n",
        "4. **otherMetrics** : Caluclates Precison, Recall, Accuracy and F1 Score.\n",
        "\n",
        "5. **getEvalutions**: Evaluates predictions with labels and returns the metrics"
      ],
      "metadata": {
        "id": "pQpCcNb4oDQh"
      },
      "id": "pQpCcNb4oDQh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4b8f83-8893-4f06-bd5d-5bce1bd7e4c8",
      "metadata": {
        "id": "fe4b8f83-8893-4f06-bd5d-5bce1bd7e4c8"
      },
      "outputs": [],
      "source": [
        "# Adds all the metrics from different oversampling techniques into table.\n",
        "def filldetails(analysisTable, predictions, sampling, model):\n",
        "    cf_matrix, ROC, accuracy, F1, precision, recall = getEvalutions(predictions)\n",
        "    print(sampling, model, ROC, accuracy, F1, precision, recall, cf_matrix)\n",
        "    analysisTable.add_row([sampling, model, ROC, accuracy, F1, precision, recall, cf_matrix])\n",
        "\n",
        "#Main method to run the specified Machine Learning models. Return Evaluation metrics. In case of Cross Validation, returns Best Model.\n",
        "def getResults(sampledData, test, isLR=False, isRF=False, isLSVC=False, isCatBoost=False, isLightGBM=False,isCV=False):\n",
        "    # Specify the Column Names while initializing the Table\n",
        "    analysisTable = PrettyTable([\"Sampling\", \"Model\", \"ROC\", \"accuracy\", \"F1\", \"precision\", \"recall\", \"Matrix\"])\n",
        "    results = {}\n",
        "    testData = getFeaturesData(test, drop=True)\n",
        "    testData.cache()\n",
        "    for sampling in sampledData:\n",
        "        print(\">>>>>>>>>>>>>>>>Started :\", sampling)\n",
        "        train = sampledData[sampling]\n",
        "        res={}\n",
        "        if isCatBoost:\n",
        "            #\"LSVC\":\n",
        "            modelDataCatBoost = CatBoost(train, testData, isCV=isCV)\n",
        "            filldetails(analysisTable, modelDataCatBoost[\"predictions\"], sampling, \"CatBoost\")\n",
        "            res[\"CatBoost\"]=modelDataCatBoost\n",
        "        if len(res.keys())>1:\n",
        "            results[sampling] = res\n",
        "        print(\"<<<<<<<<<<<<<<Finished :\", sampling)\n",
        "    return results, analysisTable\n",
        "\n",
        "# print the confusion matrix \n",
        "#USING SEABORN LIBRARY FOR VISUALIZATION PURPOSES\n",
        "def printConfusionMatrix(cf_matrix):\n",
        "    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                    cf_matrix.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "              zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n",
        "\n",
        "\n",
        "#Caluclate F1-score, Recall, Accuaracy, Precision\n",
        "def otherMetrics(cf):\n",
        "    tp = cf[0][0]\n",
        "    fp = cf[1][0]\n",
        "    fn = cf[0][1]\n",
        "    tn = cf[1][1]\n",
        "    precision = np.round((tp)/(tp+fp),3)\n",
        "    recall =  np.round((tp)/(tp+fn),3)\n",
        "    accuracy= np.round((tp+tn)/(tp+fp+fn+tn),3)\n",
        "    F1=np.round((2*precision*recall)/(precision+recall),3)\n",
        "    return accuracy, F1, precision, recall\n",
        "\n",
        "\n",
        "#Evaluate the predictions with actual labels.\n",
        "def getEvalutions(predictions):\n",
        "    evaluator=BinaryClassificationEvaluator(labelCol='label')\n",
        "    ROC = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "    preds_and_labels = predictions.withColumn('label', F.col('label').cast(FloatType())).orderBy('prediction').select(['prediction','label'])\n",
        "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
        "    cf_matrix=metrics.confusionMatrix().toArray()\n",
        "    # printConfusionMatrix(cf_matrix)\n",
        "    accuracy, F1, precision, recall= otherMetrics(cf_matrix)\n",
        "    return cf_matrix, np.round(ROC,3), accuracy, F1, precision, recall"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo \n",
        "**Training and Testing on train_sample.csv data provided along with actual Dataset.**"
      ],
      "metadata": {
        "id": "zhzUNW4ioIt7"
      },
      "id": "zhzUNW4ioIt7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03625b8a-b5fc-47f0-baf1-950a9d883acd",
      "metadata": {
        "id": "03625b8a-b5fc-47f0-baf1-950a9d883acd"
      },
      "outputs": [],
      "source": [
        "def demoData(path=\"/content/drive/MyDrive/Final Project CS 657/talkingdata-adtracking-fraud-detection/train_sample.csv\"):\n",
        "    dataDownload = spark.read\\\n",
        "      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
        "      .option(\"inferSchema\",True)\\\n",
        "      .option('header', 'true')\\\n",
        "      .load(path).drop(\"attributed_time\",\"click_time\").distinct().na.drop()\n",
        "    # Running on Full Data\n",
        "    dataNotFraud, dataFraud, ratio= findImbalance(dataDownload)\n",
        "    print(\"\\n--Undersampling to create demo set--\")\n",
        "    # Random UnderSample the big data to form processable ratio for demo.\n",
        "    underSampledData=randomUnderSamplingWithoutTransformation(dataNotFraud,dataFraud,int(ratio/8))\n",
        "    getCompleteSummary(underSampledData)\n",
        "    trainSample,testSample=stratifiedTrainTestSplit(underSampledData, ifprint=False)\n",
        "    sampledData=diffSampledData(trainSample,isUnderSample=False,isOverSample=True ,isSMOTE=False, ifprint=False)\n",
        "    results, analysisTable= getResults(sampledData,testSample, isCatBoost=True)\n",
        "    print(\"\\n________________RESULTS______________\\n\",analysisTable)\n",
        "    return results, analysisTable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultsDemo, analysisTableDemo= demoData()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE_L0gdGGuPC",
        "outputId": "733a7e23-202d-4971-886d-fc8f4ba950cb"
      },
      "id": "OE_L0gdGGuPC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Fraud: 227\n",
            "Count Not Fraud: 97693\n",
            "Ratio: 430\n",
            "\n",
            "--Undersampling to create demo set--\n",
            "Summary\n",
            "_________\n",
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+\n",
            "|summary|                ip|               app|            device|               os|           channel|      is_attributed|\n",
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+\n",
            "|  count|              2087|              2087|              2087|             2087|              2087|               2087|\n",
            "|   mean|100230.09439386679|13.959271681839962| 23.43938667944418|24.22376617153809|261.14278869190224|0.10876856732151413|\n",
            "| stddev| 77739.94495546671|17.928155573536145|260.76996773054475| 63.0199579597241|129.28617993665284|0.31142324387481285|\n",
            "|    min|                31|                 1|                 0|                0|                 3|                  0|\n",
            "|    25%|             42289|                 3|                 1|               13|               137|                  0|\n",
            "|    50%|             86812|                12|                 1|               18|               245|                  0|\n",
            "|    75%|            125175|                18|                 1|               20|               377|                  0|\n",
            "|    max|            363169|               261|              3866|              866|               497|                  1|\n",
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+\n",
            "\n",
            "_________\n",
            "Total:  2087\n",
            "Left After Dropping: 2087\n",
            "% Of Drops:  0.0\n",
            "% of drop per column\n",
            "_________\n",
            "Unique Values for each column in the table\n",
            "+----+---+------+---+-------+-------------+\n",
            "|  ip|app|device| os|channel|is_attributed|\n",
            "+----+---+------+---+-------+-------------+\n",
            "|1913| 65|    30| 64|    126|            2|\n",
            "+----+---+------+---+-------+-------------+\n",
            "\n",
            "_________\n",
            "Number of values in is_attributed for each label.\n",
            "_________\n",
            "Stats for is attributed:\n",
            "+-------------+-----+\n",
            "|is_attributed|count|\n",
            "+-------------+-----+\n",
            "|            1|  227|\n",
            "|            0| 1860|\n",
            "+-------------+-----+\n",
            "\n",
            "_________\n",
            "\n",
            "-----TRAIN TEST SPLIT STARTED----\n",
            "Count Fraud: 227\n",
            "Count Not Fraud: 1860\n",
            "Ratio: 8\n",
            "\n",
            "----SAMPLES IN TRAIN----\n",
            "Count Fraud: 174\n",
            "Count Not Fraud: 1464\n",
            "Ratio: 8\n",
            "\n",
            "----SAMPLES IN TEST-----\n",
            "Count Fraud: 53\n",
            "Count Not Fraud: 396\n",
            "Ratio: 7\n",
            "\n",
            "---Comparing data using various Sampling Techniques---\n",
            "Count Fraud: 174\n",
            "Count Not Fraud: 1464\n",
            "Ratio: 8\n",
            "\n",
            "--Random OverSampling--\n",
            ">>>>>>>>>>>>>>>>Started : randomOverSampleddata\n",
            ">>> CatBoost Invoked\n",
            "randomOverSampleddata CatBoost 0.97 0.962 0.978 0.985 0.972 [[385.  11.]\n",
            " [  6.  47.]]\n",
            "<<<<<<<<<<<<<<Finished : randomOverSampleddata\n",
            "\n",
            "________________RESULTS______________\n",
            " +-----------------------+----------+------+----------+-------+-----------+--------+---------------+\n",
            "|        Sampling       |  Model   | ROC  | accuracy |   F1  | precision | recall |     Matrix    |\n",
            "+-----------------------+----------+------+----------+-------+-----------+--------+---------------+\n",
            "| randomOverSampleddata | CatBoost | 0.97 |  0.962   | 0.978 |   0.985   | 0.972  |  [[385.  11.] |\n",
            "|                       |          |      |          |       |           |        |  [  6.  47.]] |\n",
            "+-----------------------+----------+------+----------+-------+-----------+--------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Million Records\n",
        "**Training and Testing on sampled 6 Million records from train.csv**"
      ],
      "metadata": {
        "id": "QAyfEaTRoJ3B"
      },
      "id": "QAyfEaTRoJ3B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987c915d-b952-49c5-9fe6-3ff025290568",
      "metadata": {
        "id": "987c915d-b952-49c5-9fe6-3ff025290568"
      },
      "outputs": [],
      "source": [
        "def RUN6MTEST(path=\"/content/drive/MyDrive/Final Project CS 657/talkingdata-adtracking-fraud-detection/6Million.parquet\"):\n",
        "    dataDownload=spark.read.parquet(path)\n",
        "    #getCompleteSummary(dataDownload)\n",
        "    trainSample,testSample=stratifiedTrainTestSplit(dataDownload, ifprint=False)\n",
        "    sampledData=diffSampledData(trainSample, isUnderSample=False,isOverSample=True ,isSMOTE=False, ifprint=False)\n",
        "    results, analysisTable= getResults(sampledData,testSample, isCatBoost=True)\n",
        "    print(\"\\n________________RESULTS______________\\n\",analysisTable)\n",
        "    return results, analysisTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744cb95e-306a-4762-b1e4-9af5d7b7405c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "744cb95e-306a-4762-b1e4-9af5d7b7405c",
        "outputId": "31ba9f67-1c45-47b0-a72a-2eb65eae6af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----TRAIN TEST SPLIT STARTED----\n",
            "Count Fraud: 169973\n",
            "Count Not Fraud: 446940\n",
            "Ratio: 2\n",
            "\n",
            "----SAMPLES IN TRAIN----\n",
            "Count Fraud: 136063\n",
            "Count Not Fraud: 357493\n",
            "Ratio: 2\n",
            "\n",
            "----SAMPLES IN TEST-----\n",
            "Count Fraud: 33910\n",
            "Count Not Fraud: 89447\n",
            "Ratio: 2\n",
            "\n",
            "---Comparing data using various Sampling Techniques---\n",
            "Count Fraud: 136063\n",
            "Count Not Fraud: 357493\n",
            "Ratio: 2\n",
            "\n",
            "--Random OverSampling--\n",
            ">>>>>>>>>>>>>>>>Started : randomOverSampleddata\n",
            ">>> CatBoost Invoked\n",
            "randomOverSampleddata CatBoost 0.964 0.936 0.957 0.936 0.979 [[87540.  1907.]\n",
            " [ 5988. 27922.]]\n",
            "<<<<<<<<<<<<<<Finished : randomOverSampleddata\n",
            "\n",
            "________________RESULTS______________\n",
            " +-----------------------+----------+-------+----------+-------+-----------+--------+-------------------+\n",
            "|        Sampling       |  Model   |  ROC  | accuracy |   F1  | precision | recall |       Matrix      |\n",
            "+-----------------------+----------+-------+----------+-------+-----------+--------+-------------------+\n",
            "| randomOverSampleddata | CatBoost | 0.964 |  0.936   | 0.957 |   0.936   | 0.979  |  [[87540.  1907.] |\n",
            "|                       |          |       |          |       |           |        |  [ 5988. 27922.]] |\n",
            "+-----------------------+----------+-------+----------+-------+-----------+--------+-------------------+\n"
          ]
        }
      ],
      "source": [
        "results6M, analysisTable6M= RUN6MTEST()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 26 Million Records\n",
        "**Training and Testing on sampled 26 Million records from train.csv**\n"
      ],
      "metadata": {
        "id": "eV3wF56ZoKi4"
      },
      "id": "eV3wF56ZoKi4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4b3a46-b9f6-4241-a79d-fc99bf60ef80",
      "metadata": {
        "id": "4c4b3a46-b9f6-4241-a79d-fc99bf60ef80"
      },
      "outputs": [],
      "source": [
        "def RUN26MTEST(path=\"/content/drive/MyDrive/Final Project CS 657/talkingdata-adtracking-fraud-detection/26Million.parquet\"):\n",
        "    dataDownload=spark.read.parquet(path)\n",
        "    getCompleteSummary(dataDownload)\n",
        "    trainSample,testSample=stratifiedTrainTestSplit(dataDownload, ifprint=False)\n",
        "    sampledData=diffSampledData(trainSample, isUnderSample=True,isOverSample=True, ifprint=False)\n",
        "    results, analysisTable= getResults(sampledData,testSample, isCatBoost=True)\n",
        "    print(\"\\n________________RESULTS______________\\n\",analysisTable)\n",
        "    return results, analysisTable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results26M, analysisTable26M= RUN26MTEST()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6_nARQiaZwu",
        "outputId": "f2fa7062-de1c-4928-ba75-dfcc23660fc8"
      },
      "id": "l6_nARQiaZwu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary\n",
            "_________\n",
            "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------+\n",
            "|summary|                ip|              app|            device|                os|           channel|      is_attributed|\n",
            "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------+\n",
            "|  count|           2570962|          2570962|           2570962|           2570962|           2570962|            2570962|\n",
            "|   mean|117578.32723781993|15.83455609223318|20.505753099423483|24.018647883554873| 269.1028957254133|0.16480679216573407|\n",
            "| stddev|  88516.5227112953|21.98906805298176|235.46908212340674|  54.1577769104347|137.00831860255715|0.37100615488199506|\n",
            "|    min|                 1|                0|                 0|                 0|                 0|                  0|\n",
            "|    25%|             48418|                3|                 1|                13|               135|                  0|\n",
            "|    50%|             97919|               12|                 1|                18|               245|                  0|\n",
            "|    75%|            170321|               18|                 1|                25|               402|                  0|\n",
            "|    max|            364778|              768|              4225|               916|               498|                  1|\n",
            "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------+\n",
            "\n",
            "_________\n",
            "Total:  2570962\n",
            "Left After Dropping: 2570962\n",
            "% Of Drops:  0.0\n",
            "% of drop per column\n",
            "_________\n",
            "Unique Values for each column in the table\n",
            "+------+---+------+---+-------+-------------+\n",
            "|    ip|app|device| os|channel|is_attributed|\n",
            "+------+---+------+---+-------+-------------+\n",
            "|265843|424|  1999|307|    182|            2|\n",
            "+------+---+------+---+-------+-------------+\n",
            "\n",
            "_________\n",
            "Number of values in is_attributed for each label.\n",
            "_________\n",
            "Stats for is attributed:\n",
            "+-------------+-------+\n",
            "|is_attributed|  count|\n",
            "+-------------+-------+\n",
            "|            1| 423712|\n",
            "|            0|2147250|\n",
            "+-------------+-------+\n",
            "\n",
            "_________\n",
            "\n",
            "-----TRAIN TEST SPLIT STARTED----\n",
            "Count Fraud: 423712\n",
            "Count Not Fraud: 2147250\n",
            "Ratio: 5\n",
            "\n",
            "----SAMPLES IN TRAIN----\n",
            "Count Fraud: 339191\n",
            "Count Not Fraud: 1718509\n",
            "Ratio: 5\n",
            "\n",
            "----SAMPLES IN TEST-----\n",
            "Count Fraud: 84521\n",
            "Count Not Fraud: 428741\n",
            "Ratio: 5\n",
            "\n",
            "---Comparing data using various Sampling Techniques---\n",
            "Count Fraud: 339191\n",
            "Count Not Fraud: 1718509\n",
            "Ratio: 5\n",
            "\n",
            "--Undersampling--\n",
            "\n",
            "--Random OverSampling--\n",
            ">>>>>>>>>>>>>>>>Started : underSampledData\n",
            ">>> CatBoost Invoked\n",
            "underSampledData CatBoost 0.964 0.935 0.96 0.974 0.947 [[406087.  22654.]\n",
            " [ 10813.  73708.]]\n",
            "<<<<<<<<<<<<<<Finished : underSampledData\n",
            ">>>>>>>>>>>>>>>>Started : randomOverSampleddata\n",
            ">>> CatBoost Invoked\n",
            "randomOverSampleddata CatBoost 0.964 0.949 0.969 0.969 0.97 [[415764.  12977.]\n",
            " [ 13421.  71100.]]\n",
            "<<<<<<<<<<<<<<Finished : randomOverSampleddata\n",
            "\n",
            "________________RESULTS______________\n",
            " +-----------------------+----------+-------+----------+-------+-----------+--------+---------------------+\n",
            "|        Sampling       |  Model   |  ROC  | accuracy |   F1  | precision | recall |        Matrix       |\n",
            "+-----------------------+----------+-------+----------+-------+-----------+--------+---------------------+\n",
            "|    underSampledData   | CatBoost | 0.964 |  0.935   |  0.96 |   0.974   | 0.947  |  [[406087.  22654.] |\n",
            "|                       |          |       |          |       |           |        |  [ 10813.  73708.]] |\n",
            "| randomOverSampleddata | CatBoost | 0.964 |  0.949   | 0.969 |   0.969   |  0.97  |  [[415764.  12977.] |\n",
            "|                       |          |       |          |       |           |        |  [ 13421.  71100.]] |\n",
            "+-----------------------+----------+-------+----------+-------+-----------+--------+---------------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CZq4DPiygaKq",
        "l9kaZR38cYDo",
        "FQQgTTszfris",
        "rlaSrb1riVhh",
        "t-C5WKtMnkNv",
        "zhzUNW4ioIt7",
        "QAyfEaTRoJ3B",
        "eV3wF56ZoKi4"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}